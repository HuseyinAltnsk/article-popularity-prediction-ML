
\section{Conclusions}
\label{sec:concl}

In this paper, we set out to build a model that could accurately predict the popularity of news articles in terms of momentum scores. After converting text data in to trainable,numerical values, we built and trained our models, and  discovered that \texttt{LinearSVR} came away with the best results.Our most important discovery is that increasing the number of trees in the \texttt{RandomForestRegressor} model mitigated the high variance problem we saw in our previous results by a fair amount. Perhaps using a large number of trees is required for this problem, given the immense feature size we had.\\


Should we have more time in the future, we would like to go a few steps further with pre-processing our data. First of all, we would like to add a few meta data properties to be the feature data. Recall that for this project, we only considered the text content of individual articles as our feature. However, meta data such as the specific website where an article is published, its author, or even the time it was published can potentially affect its popularity too. Secondly, we would like to make sure all duplicates are deleted for a less biased result. The \texttt{panda}'s drop\_duplicates function allowed us to throw away articles that have the same titles. However, we found out later in our project that this approach didn't do the entire job as some articles had the exact same contents and momentum scores, but had different titles. Lastly, we would trim down our data set even more and take away more words that do not contribute to the popularity measure. For this project, we only took out common English words like ``a, the, it", etc. However, we believe that only a small number of words should matter for the popularity measure, and sometimes only one keyword such as \textit{Donald} could be responsible for a high score. \\
